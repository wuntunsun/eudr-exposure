{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# maps\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# prep\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# feature engineering\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# modelling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "forest = '#284e13ff'\n",
    "\n",
    "# xgb_cv_r2 = {x: np.nan for x in list(range(5))}\n",
    "# xgb_cv_mse = {x: np.nan for x in list(range(5))}\n",
    "# xgb_r2 = {x: np.nan for x in list(range(5))}\n",
    "# xgb_mse = {x: np.nan for x in list(range(5))}\n",
    "# sample_size = {x: np.nan for x in list(range(5))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas_list = ['', '_1950', '_2910', '_4830', '_7710']\n",
    "\n",
    "############ PICK AREA SIZE HERE ##############\n",
    "area = areas_list[4]\n",
    "###############################################\n",
    "\n",
    "file_to_load = 'data/regression_sample' + area + '.csv'\n",
    "index = areas_list.index(area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_to_load, sep = \"\\t\")\n",
    "\n",
    "# Define a dictionary with the features to convert to 'category'\n",
    "to_category = {'sector_main': 'category', 'country': 'category', 'start_year_first': 'category'}\n",
    "\n",
    "# Convert the features to 'category' dtype\n",
    "df = df.astype(to_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_means = df[df.defo_total > 0].groupby('sector_main').around_3.describe().reset_index()[['sector_main', 'mean', 'count']]\n",
    "\n",
    "row_total = {'sector_main': 'total', 'mean':df[df.defo_total > 0].around_3.mean(), 'count': df[df.defo_total > 0].around_3.count()}\n",
    "row_total_df = pd.DataFrame([row_total])\n",
    "\n",
    "sector_means = pd.concat([sector_means, row_total_df], ignore_index=True).sort_values('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "colors = [forest] * len(sector_means)\n",
    "colors[9] = '#c83215'\n",
    "\n",
    "plt.barh(sector_means.sector_main, sector_means['mean'], color = colors)\n",
    "plt.title('Mean asset deforestation by sector, t = (-1,1)')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('graphs/hbar_means_by_sector.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_counts = df.groupby('sector_main').around_3.describe().reset_index()[['sector_main', 'mean', 'count']].sort_values('count')\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "\n",
    "plt.barh(sector_counts.sector_main, sector_counts['count'], color = forest)\n",
    "plt.title('Number of assets, by sector')\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('graphs/hbar_sectors.png', bbox_inches='tight')\n",
    "df[(df.treecover2000 > 0) & (df.defo_total > 0)].sector_main.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var = 'around_3'\n",
    "\n",
    "df_pred = df[df[y_var].notnull()]\n",
    "df_pred = df_pred[df_pred.treecover2000 > 0]\n",
    "# df_pred = df_pred[df_pred.start_year_first < 2013]\n",
    "df_pred = df_pred[df_pred.defo_total > 0]\n",
    "\n",
    "aux_country = df_pred.groupby('country').uid_gem.count().reset_index().sort_values('uid_gem').rename(columns = {'uid_gem': 'country_count'})\n",
    "df_pred = pd.merge(df_pred, aux_country, how = 'inner', on = 'country')\n",
    "\n",
    "df_pred = df_pred[df_pred.country_count > 1]\n",
    "\n",
    "plt.hist(df_pred.around_3, bins = 20)\n",
    "df_pred.describe()\n",
    "\n",
    "# df.groupby('quintile_capacity').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = ['sector_main', 'country', 'number_units', 'start_year_first'] #, 'defo_total']\n",
    "X = df_pred[X_cols]\n",
    "X_strat = df_pred[['country']]\n",
    "y = df_pred[y_var].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify = X_strat)\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size[index] = len(X_train)\n",
    "plt.hist(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ColumnTransformer to handle string data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('country', OneHotEncoder(), ['country']),\n",
    "        # ('sector_sub_first', OneHotEncoder(), ['sector_sub_first']),\n",
    "        ('sector_main', OneHotEncoder(), ['sector_main']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model: mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(y_train)\n",
    "\n",
    "y_pred = [mean] * len(y_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "baseline_string = f\"Baseline output: r2 of {round(r2, 5)}, mse of {round(mse, 6)}\"\n",
    "print(baseline_string)\n",
    "# y_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "lm = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "knn = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbm = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', XGBRegressor(enable_categorical = True))\n",
    "])\n",
    "xgbm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning rate (Greek letter eta) controls the step size at which the optimizer makes updates to the weights. \n",
    "# A smaller eta value results in slower but more accurate updates, while a larger eta value results \n",
    "# in faster but less accurate updates.\n",
    "\n",
    "# The subsample parameter controls the fraction of observations used for each tree. A smaller subsample \n",
    "# value results in smaller and less complex models, which can help prevent overfitting.\n",
    "\n",
    "# For any given problem, a lower log loss value means better predictions.\n",
    "\n",
    "# You don't need xgboost.cv to find the optimal number of trees. You can also \n",
    "# run xgboost.train with \"early_stopping_rounds\" set. \n",
    "\n",
    "X_train_t = preprocessor.fit_transform(X_train, y_train)\n",
    "X_test_t = preprocessor.fit_transform(X_test, y_test)\n",
    "\n",
    "#xgb.callback.TrainingCallback()\n",
    "\n",
    "# res.best_iteration\n",
    "# is most likely to be the last one but maybe not...\n",
    "\n",
    "# n_estimators\n",
    "# num_boosting_rounds\n",
    "\n",
    "# Tune max_depth and min_child_weight first as they will have the highest impact on the model outcome.\n",
    "\n",
    "# could provide a DataFrame with all desired combinations...\n",
    "# then keep track of the results to choose the best...\n",
    "\n",
    "# def fit(xgbm, **kwargs):\n",
    "\n",
    "#     data_dmatrix = xgb.DMatrix(data=X_train_t, label=y_train)\n",
    "\n",
    "#     # other parameters include max_depth, min_child_weight...\n",
    "#     # defaults are all within a good range\n",
    "\n",
    "#     # run cv for each model...\n",
    "#     # GridSearchCV could be useful...\n",
    "\n",
    "#     params = {'objective':'reg:squarederror',\n",
    "#               'eval_metric':'rmse',\n",
    "#               'eta': x[0],\n",
    "#               'subsample':x[1]}\n",
    "#     # stratified=True ???\n",
    "#     # num_boost_round can be set high, \n",
    "#     xgb_cv = xgb.cv(dtrain=data_dmatrix, \n",
    "#                     params=params, \n",
    "#                     nfold=5, \n",
    "#                     early_stopping_rounds=50, # over all folds...\n",
    "#                     metrics = 'rmse', \n",
    "#                     seed=42)\n",
    "#     print(xgb_cv.shape)\n",
    "#     print(xgb_cv)\n",
    "#     return xgb_cv[-1:].values[0]\n",
    "\n",
    "# grid = pd.DataFrame({'eta':[0.01,0.05,0.1]*2, \n",
    "#                      'subsample':np.repeat([0.1,0.3],3),\n",
    "#                      })\n",
    "\n",
    "# grid[['train-rmse-mean','train-rmse-std',\n",
    "#     'test-rmse-mean','test-rmse-std']] = grid.apply(fit, axis=1, result_type='expand')\n",
    "\n",
    "# params = {'objective':'reg:squarederror',\n",
    "#             'eval_metric':'rmse',\n",
    "#             'eta': grid.iloc[4].eta,\n",
    "#             'subsample': grid.iloc[4].subsample}\n",
    "# regressor = XGBRegressor(**params)\n",
    "\n",
    "def fit_for_early_stopping(boost, X_train, y_train, cv = 5, early_stopping_rounds = 50):\n",
    "    params = boost.get_params()\n",
    "    xgtrain = xgb.DMatrix(X_train, y_train)\n",
    "    cvresult = xgb.cv(boost.get_xgb_params(), xgtrain, num_boost_round=params['n_estimators'], nfold=cv,\n",
    "            metrics='rmse', early_stopping_rounds=early_stopping_rounds)\n",
    "    boost.set_params(n_estimators = cvresult.shape[0])\n",
    "    print(cvresult.shape[0])\n",
    "\n",
    "xgbr = XGBRegressor(\n",
    "    learning_rate = 0.1, # eta\n",
    "    n_estimators = 1000,\n",
    "    max_depth=7,\n",
    "    #min_child_weight=1,\n",
    "    gamma = 0,\n",
    "    #subsample=0.8,\n",
    "    #colsample_bytree=0.8,\n",
    "    #objective= 'reg:squarederror',\n",
    "    seed = 42, \n",
    "    enable_categorical = True)\n",
    "\n",
    "# fit n_estimators on training data for early stopping\n",
    "fit_for_early_stopping(xgbr, X_train_t, y_train, cv = 5, early_stopping_rounds = 50)\n",
    "\n",
    "param_grid = {\n",
    "    'eta': [0.00001, 0.0001, 0.001, 0.01], #0.0001, 0.001,\n",
    "    'subsample': [0.1, 0.2, 0.3, 0.4, 0.5], \n",
    "    'min_child_weight': [0.001, 0.01, 0.05, 0.1, 0.5],\n",
    "    'max_depth': list(range(5, 10, 2)),\n",
    "    'gamma': [0],\n",
    "    'alpha': [0, 1, 10, 100],\n",
    "}\n",
    "\n",
    "gridsearch = GridSearchCV(xgbr, param_grid, verbose = 0, cv = 5, n_jobs = -1)\n",
    "\n",
    "gridsearch.fit(X_train_t, y_train)\n",
    "\n",
    "xgbm2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', gridsearch.best_estimator_)\n",
    "])\n",
    "xgbm2.fit(X_train, y_train)\n",
    "\n",
    "gridsearch.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = lm.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Linreg output: r2 of {round(r2, 3)}, mse of {round(mse, 6)}\")\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# print(\"----\" * 10)\n",
    "\n",
    "# y_pred = knn.predict(X_test)\n",
    "\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# print(f\"KNN output: r2 of {round(r2, 3)}, mse of {round(mse, 6)}\")\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# print(\"----\" * 10)\n",
    "\n",
    "print(\"XGB:\")\n",
    "\n",
    "y_pred = xgbm.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"XGB output: r2 of {round(r2, 3)}, mse of {round(mse, 6)}\")\n",
    "\n",
    "xgb_r2[index] = round(r2, 5)\n",
    "xgb_mse[index] = round(mse, 5)\n",
    "\n",
    "\n",
    "print(\"--- on train ---\")\n",
    "\n",
    "y_pred = xgbm.predict(X_train)\n",
    "mse = mean_squared_error(y_train, y_pred)   \n",
    "r2 = r2_score(y_train, y_pred)\n",
    "print(f\"r2 of {round(r2, 3)}, mse of {round(mse, 6)}\")\n",
    "\n",
    "print(\"XGB (cv):\")\n",
    "\n",
    "y_pred = xgbm2.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"XGB (cv) output: r2 of {round(r2, 3)}, mse of {round(mse, 6)}\")\n",
    "\n",
    "xgb_cv_r2[index] = round(r2, 5)\n",
    "xgb_cv_mse[index] = round(mse, 5) \n",
    "\n",
    "\n",
    "print(\"--- on train ---\")\n",
    "\n",
    "y_pred = xgbm2.predict(X_train)\n",
    "mse = mean_squared_error(y_train, y_pred)   \n",
    "r2 = r2_score(y_train, y_pred)\n",
    "print(f\"r2 of {round(r2, 3)}, mse of {round(mse, 6)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"XGB CV MSE: {xgb_cv_mse}\")\n",
    "print(f\"XGB CV R2: {xgb_cv_r2}\")\n",
    "print(\"------\" * 10)\n",
    "print(f\"XGB MSE: {xgb_mse}\")\n",
    "print(f\"XGB R2: {xgb_r2}\")\n",
    "print(\"------\" * 10)\n",
    "print(f\"Sample size: {sample_size}\")\n",
    "print(baseline_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
